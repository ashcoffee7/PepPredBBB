# -*- coding: utf-8 -*-
"""PepPredBBB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KAuFSnJVs2aZYiIE07NP3rZvb4b16JxF

# **importing sequences**
"""

pos = "/content/drive/MyDrive/pepfun/Training_format_pos (1).txt"
neg = "/content/drive/MyDrive/pepfun/Training_format_neg.txt"

# pos sequences extract into list
f = open(pos, 'r')
file_contents = f.read()
data = file_contents
f.close()

newdatapos = data.splitlines()
print(newdatapos)

# neg sequences extract into list
f2 = open(neg, 'r')
file_contents2 = f2.read()
data2 = file_contents2
f2.close()

newdataneg = data2.splitlines()
print(newdataneg)

"""# **fingerprints**"""

!pip install rdkit-pypi
import rdkit
from rdkit import Chem

# fingerprints for pos sequences

from rdkit.Chem.AtomPairs import Pairs
from rdkit.Chem import DataStructs
import numpy as np


fingerprintpos = []
for item in newdatapos:
  converteditem = Chem.MolFromFASTA(item)
  atompos = Pairs.GetAtomPairFingerprint(converteditem)
  array = np.zeros((0,), dtype=np.int8)
  DataStructs.ConvertToNumpyArray(atompos, array)
  r = array.nonzero()
  fingerprintpos.append(r)

print(len(fingerprintpos))

# fingerprints for neg sequences
fingerprintneg = []
for item in newdataneg:
  converteditemn = Chem.MolFromFASTA(item)
  atomneg = Pairs.GetAtomPairFingerprint(converteditemn)
  arrayn = np.zeros((0,), dtype=np.int8)
  DataStructs.ConvertToNumpyArray(atomneg, arrayn)
  rn = arrayn.nonzero()
  fingerprintneg.append(rn)

print(len(fingerprintneg))

allfingerprints = fingerprintpos + fingerprintneg
print(len(allfingerprints))

arrayfingerprints = np.array(allfingerprints)

"""# **make embeddings**"""

import numpy as np
import string
import pandas as pd

# set up embeddings
import nltk
from gensim.models import Word2Vec
import multiprocessing
EMB_DIM = 4

# embeddings pos
w2vpos = Word2Vec([newdatapos], size=EMB_DIM, min_count=1)
sequez = "VVYPWTQRF"
w2vpos[sequez].shape
words=list(w2vpos.wv.vocab)
vectors = []
for word in words:
  vectors.append(w2vpos[word].tolist())
print(len(vectors))
print(vectors[1])
data = np.array(vectors)

# embeddings neg
w2vneg = Word2Vec([newdataneg], size=EMB_DIM, min_count=1)
sequen = "GIGKFLHSAGKFGKAFLGEVMKS"
w2vneg[sequen].shape
wordsneg = list(w2vneg.wv.vocab)
vectorsneg = []
for word in wordsneg:
  vectorsneg.append(w2vneg[word].tolist())
allvectors = vectorsneg + vectors
print(len(allvectors))
arrayvectors = np.array(allvectors)

labels = []
for i in range (100):
  labels.append(1)
print(labels)
for i in range (100):
  labels.append(0)
print(labels)
print(len(labels))


labelsval = []
for i in range (19):
  labelsval.append(1)
for i in range (19):
  labelsval.append(0)

"""# **make model**"""

import seaborn as sns
!pip install keras
import keras
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils import shuffle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
!pip install tensorflow==2.7.0
import tensorflow as tf
from keras import metrics
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Conv3D, Flatten, Dropout

import sklearn
a = sklearn.utils.shuffle(arrayvectors, random_state=1)
b = sklearn.utils.shuffle(labels, random_state=1)
c = sklearn.utils.shuffle(arrayfingerprints, random_state =1 )
dfa = pd.DataFrame(a, c, columns=None)
dfb = pd.DataFrame(b, columns=None)
X = dfa.iloc[:]
y = dfb.iloc[:]

from sklearn.datasets import make_classification
X, y = make_classification(n_classes=2, n_features=4, n_informative=4, n_redundant=0, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=300)
X_train = np.asarray(X_train)
X_test = np.asarray(X_test)
y_train = np.asarray(y_train)
y_test = np.asarray(y_test)
y_train = y_train.astype(np.float32)
y_test = y_test.astype(np.float32)

# train data & test data tensor conversion

class trainData(Dataset):

    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    def __len__ (self):
        return len(self.X_data)


train_data = trainData(torch.FloatTensor(X_train),
                       torch.FloatTensor(y_train))
## test data
class testData(Dataset):

    def __init__(self, X_data):
        self.X_data = X_data

    def __getitem__(self, index):
        return self.X_data[index]

    def __len__ (self):
        return len(self.X_data)


test_data = testData(torch.FloatTensor(X_test))

EPOCHS = 100
BATCH_SIZE = 2
LEARNING_RATE = 0.0001

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=1)

# make model
model = Sequential()
model.add(Dense(5, activation='relu', input_shape=(4,)))
model.add(Dropout(0.1))
model.add(Dense(1,activation='sigmoid'))
model.summary()

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())

def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())

def mcc_metric(y_true, y_pred):
  predicted = tf.cast(tf.greater(y_pred, 0.5), tf.float32)
  true_pos = tf.math.count_nonzero(predicted * y_true)
  true_neg = tf.math.count_nonzero((predicted - 1) * (y_true - 1))
  false_pos = tf.math.count_nonzero(predicted * (y_true - 1))
  false_neg = tf.math.count_nonzero((predicted - 1) * y_true)
  x = tf.cast((true_pos + false_pos) * (true_pos + false_neg)
      * (true_neg + false_pos) * (true_neg + false_neg), tf.float32)
  return tf.cast((true_pos * true_neg) - (false_pos * false_neg), tf.float32) / tf.sqrt(x)

model.compile(loss='binary_crossentropy',optimizer='adam', metrics=[sensitivity, mcc_metric, specificity, 'accuracy','AUC', precision_m, f1_m, recall_m])

history = model.fit(X_train, y_train, epochs=100,batch_size=6, validation_data = (X_test, y_test), validation_batch_size=64)
# extract the predicted probabilities
p_pred = model.predict(X_test)
p_pred = p_pred.flatten()
print(p_pred.round(2))
# [1. 0.01 0.91 0.87 0.06 0.95 0.24 0.58 0.78 ...

# extract the predicted class labels
y_pred = np.where(p_pred > 0.5, 1, 0)
print(y_pred)
# [1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 ...

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test,y_pred))

print(confusion_matrix(y_test, y_pred))
cm = confusion_matrix(y_test,y_pred)

import seaborn as sns
sns.heatmap(cm, annot=True, cmap="Blues")

from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

from sklearn.metrics import auc
auc = auc(fpr, tpr)
print(auc)

plt.plot(fpr, tpr)
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.show()

"""# **logistic regression**"""

sequencenumber = []
for i in range (1,201):
  sequencenumber.append(i)

print(sequencenumber)

# Commented out IPython magic to ensure Python compatibility.
#Import libraries, features and settings (not all of these are needed so pull what you need)

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import io
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn import preprocessing
plt.rc("font", size = 14)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
sns.set(style="white")
sns.set(style="whitegrid", color_codes = True)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
X_train2, X_test2, y_train2, y_test2 = train_test_split(X,y,test_size=0.2, random_state=300)
logreg.fit(X_train2,y_train2)
y_pred2 = logreg.predict(X_test2)

from sklearn import metrics
cnf_matrix2 = metrics.confusion_matrix(y_test2, y_pred2)
cnf_matrix2

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix2), annot=True, cmap="Blues")
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print("Accuracy:",metrics.accuracy_score(y_test2, y_pred2))
print("Precision:",metrics.precision_score(y_test2, y_pred2))
print("Recall:",metrics.recall_score(y_test2, y_pred2))

y_pred_proba = logreg.predict_proba(X_test2)[::,1]
fpr1, tpr1, _ = metrics.roc_curve(y_test2,  y_pred_proba)
auc1 = metrics.roc_auc_score(y_test2, y_pred_proba)
plt.plot(fpr1,tpr1,label="data 1, auc="+str(auc1))
plt.legend(loc=4)
plt.show()

plt.plot(fpr, tpr, color='g',label="data 1, auc="+str(auc))
plt.plot(fpr1,tpr1,color='r',label="data 1, auc1="+str(auc1))
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc=4)

plt.show()

"""# **without embeddings**"""



import seaborn as sns
!pip install keras
import keras
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils import shuffle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
!pip install tensorflow==2.7.0
import tensorflow as tf
from keras import metrics
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Conv3D, Flatten, Dropout

import sklearn
a1 = sklearn.utils.shuffle(arrayvectors, random_state=1)
b1 = sklearn.utils.shuffle(labels, random_state=1)
dfa1 = pd.DataFrame(a, columns=None)
dfb1 = pd.DataFrame(b, columns=None)
X3 = dfa.iloc[:]
y3 = dfb.iloc[:]

from sklearn.datasets import make_classification
X3, y3 = make_classification(n_classes=2, n_features=4, n_informative=4, n_redundant=0, random_state=42)

X3_train, X3_test, y3_train, y3_test = train_test_split(X3,y3,test_size=0.2, random_state=300)
X3_train = np.asarray(X3_train)
X3_test = np.asarray(X3_test)
y3_train = np.asarray(y3_train)
y3_test = np.asarray(y3_test)
y3_train = y3_train.astype(np.float32)
y3_test = y3_test.astype(np.float32)

# train data & test data tensor conversion

class trainData(Dataset):

    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    def __len__ (self):
        return len(self.X_data)


train_data3 = trainData(torch.FloatTensor(X3_train),
                       torch.FloatTensor(y3_train))
## test data
class testData(Dataset):

    def __init__(self, X_data):
        self.X_data = X_data

    def __getitem__(self, index):
        return self.X_data[index]

    def __len__ (self):
        return len(self.X_data)


test_data3 = testData(torch.FloatTensor(X3_test))

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=1)

# make mode
model3 = Sequential()
model3.add(Dense(5, activation='relu', input_shape=(4,)))
model3.add(Dropout(0.1))
model3.add(Dense(1,activation='sigmoid'))
model3.summary()
from keras.utils.vis_utils import plot_model

plot_model(model3, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())

def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())

model3.compile(loss='binary_crossentropy',optimizer='adam', metrics=[sensitivity, specificity, 'accuracy','AUC', precision_m, f1_m, recall_m])

history = model3.fit(X3_train, y3_train, epochs=100,batch_size=16, validation_data = (X3_test, y3_test), validation_batch_size=64)
# extract the predicted probabilities
p2_pred = model3.predict(X3_test)
p2_pred = p2_pred.flatten()
print(p2_pred.round(2))
# [1. 0.01 0.91 0.87 0.06 0.95 0.24 0.58 0.78 ...

# extract the predicted class labels
y3_pred = np.where(p2_pred > 0.5, 1, 0)
print(y3_pred)
# [1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 ...

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y3_test,y3_pred))

print(confusion_matrix(y3_test, y3_pred))
cm = confusion_matrix(y3_test,y3_pred)



import seaborn as sns
sns.color_palette("crest", as_cmap=True)
sns.heatmap(cm, cmap="Blues", annot=True)